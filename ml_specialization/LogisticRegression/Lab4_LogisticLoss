import numpy as np
import matplotlib.pyplot as plt
from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example
from plt_logistic_loss import soup_bowl, plt_logistic_squared_error
plt.style.use('./deeplearning.mplstyle')

# This is what mean squared error looks like for LINEAR regression
# soup_bowl()

x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)
y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)
plt_simple_example(x_train, y_train)

plt.close('all')
# Now, let's get a surface plot of the cost using a squared error cost:
# ğ½(ğ‘¤,ğ‘)=12ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2
# where
# ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘¤ğ‘¥(ğ‘–)+ğ‘)
# You should see that the resulting plot is not convex anymore and has several local minima.
# Hence the same cost function of ğ½(ğ‘¤,ğ‘)=12ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))âˆ’ğ‘¦(ğ‘–))2 cannot be used for logistic regression.
plt_logistic_squared_error(x_train,y_train)
# plt.show()

# Now, let's get a surface plot of the cost using a logistic loss cost:
# This is defined:

# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))
#   is the cost for a single data point, which is:
# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))= âˆ’log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))   if ğ‘¦(ğ‘–)=1
# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))= âˆ’log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–))) if ğ‘¦(ğ‘–)=0
 
# ğ‘“ğ°,ğ‘(ğ±(ğ‘–))
#   is the model's prediction, while  ğ‘¦(ğ‘–)
#   is the target value.

# ğ‘“ğ°,ğ‘(ğ±(ğ‘–))=ğ‘”(ğ°â‹…ğ±(ğ‘–)+ğ‘)
#   where function  ğ‘”
#   is the sigmoid function.
# Here you can see that the cost is convex and has only one global minimum.
# Uncomment to test below
# plt_two_logistic_loss_curves()


# The loss function above can be rewritten to be easier to implement.
# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))=(âˆ’ğ‘¦(ğ‘–)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))
 
# This is a rather formidable-looking equation. It is less daunting when you consider  ğ‘¦(ğ‘–)
#   can have only two values, 0 and 1. One can then consider the equation in two pieces:
# when  ğ‘¦(ğ‘–)=0
#  , the left-hand term is eliminated:
# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),0)=(âˆ’(0)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’0)log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))=âˆ’log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))
 
# and when  ğ‘¦(ğ‘–)=1
#  , the right-hand term is eliminated:
# ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),1)=(âˆ’(1)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’1)log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))=âˆ’log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))

plt.close('all')
cst = plt_logistic_cost(x_train,y_train)
print("Cost is: ", cst)

