import numpy as np
import matplotlib.pyplot as plt
from plt_overfit import overfit_example, output
from lab_utils_common import sigmoid
import IPython
np.set_printoptions(precision=8)

# Cost function for regularized linear regression
# The equation for the cost function regularized linear regression is:

# 𝐽(𝐰,𝑏)=1/2𝑚∑(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))^2 + 𝜆/2𝑚∑𝑤2_j {j=0..𝑛−1 & i=0..𝑚-1}
# where 𝜆 is the regularization parameter and 𝑓𝐰,𝑏(𝐱(𝑖))=𝐰⋅𝐱(𝑖)+𝑏 is the linear regression model.
# Compare this to the cost function without regularization (which you implemented in a previous lab), which is of the form:

# 𝐽(𝐰,𝑏)=1/2𝑚∑(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))2  {i=0..𝑚-1}}
# The difference is the regularization term,  𝜆/2𝑚∑𝑤2_𝑗 {j=0..𝑛−1}

# The regularization term is added to the cost function to prevent overfitting.

def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m  = X.shape[0]
    n  = len(w)
    cost = 0.
    for i in range(m):
        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot
        cost = cost + (f_wb_i - y[i])**2                               #scalar             
    cost = cost / (2 * m)                                              #scalar  
 
    # Add regularization term
    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar

# Test your implementation of the cost function with regularization
np.random.seed(1)
X_tmp = np.random.rand(5,6)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5
b_tmp = 0.5
lambda_tmp = 0.7
cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)

print("Regularized cost:", cost_tmp)



# Cost function for regularized logistic regression
# For regularized logistic regression, the cost function is of the form
# 𝐽(𝐰,𝑏) = 1/𝑚∑ [−𝑦(𝑖)log(𝑓𝐰,𝑏(𝐱(𝑖))) − (1−𝑦(𝑖))log(1−𝑓𝐰,𝑏(𝐱(𝑖))) ] + 𝜆/2𝑚∑𝑤2_𝑗 {j=0..𝑛−1 & i=0..𝑚-1}
# where 𝜆 is the regularization parameter and 𝑓𝐰,𝑏(𝐱(𝑖))=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝐰⋅𝐱(𝑖)+𝑏) is the logistic regression model.
#  the difference is the regularization term, which is  𝜆/2𝑚∑𝑤2_𝑗 {j=0..𝑛−1}
def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m,n  = X.shape
    cost = 0.
    for i in range(m):
        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot
        f_wb_i = sigmoid(z_i)                                          #scalar
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar
             
    cost = cost/m                                                      #scalar

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar

# Test your implementation of the cost function with regularization
np.random.seed(1)
X_tmp = np.random.rand(5,6)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5
b_tmp = 0.5
lambda_tmp = 0.7
cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)
print("Regularized cost:", cost_tmp)


# Gradient descent with regularization
# Computing the Gradient with regularization (both linear/logistic)
# The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of  𝑓𝐰𝑏
# dj_dw_j = 1/m * sum( (f_wb_i - y_i) * x_i_j ) + lambda/m * w_j
# dj_db = 1/m * sum( (f_wb_i - y_i) )

# For a linear regression model
# 𝑓𝐰,𝑏(𝑥)=𝐰⋅𝐱+𝑏
 
# For a logistic regression model
# 𝑧=𝐰⋅𝐱+𝑏
# 𝑓𝐰,𝑏(𝑥)=𝑔(𝑧)
# where  𝑔(𝑧) is the sigmoid function:
# 𝑔(𝑧)=11+𝑒−𝑧

# Gradient descent for linear regression with regularization

def compute_gradient_linear_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]                 
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]               
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m   
    
    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw

def compute_gradient_logistic_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns
      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))                            #(n,)
    dj_db = 0.0                                       #scalar

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]                       #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   #(n,)
    dj_db = dj_db/m                                   #scalar

    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw  

# Test your implementation of the gradient function with regularization for linear regression
np.random.seed(1)
X_tmp = np.random.rand(5,3)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1])
b_tmp = 0.5
lambda_tmp = 0.7
dj_db_tmp, dj_dw_tmp = compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)
print("dj_db for linear regression:", dj_db_tmp)
print("Regularized dj_dw for linear regression:", dj_dw_tmp)

# Test your implementation of the gradient function with regularization for logistic regression
np.random.seed(1)
X_tmp = np.random.rand(5,3)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1])
b_tmp = 0.5
lambda_tmp = 0.7
dj_db_tmp, dj_dw_tmp = compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)
print("dj_db for logistic regression:", dj_db_tmp)
print("Regularized dj_dw for logistic regression:", dj_dw_tmp)

plt.close("all")
IPython.display.display(output)
ofit = overfit_example(True)
plt.show()

